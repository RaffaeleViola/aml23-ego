{
<<<<<<< HEAD
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started for the AML 2023/2024 Egocentric Vision Project"
   ]
=======
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting started for the AML 2023/2024 Egocentric Vision Project"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EgovisionPolito/aml23-ego/blob/master/colab_runner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the public repository (or your repository)\n",
        "!git clone --branch Nunzio https://github.com/RaffaeleViola/aml23-ego.git aml23-ego1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.17.7-py3-none-win_amd64.whl.metadata (10 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting PyYAML>=5.1.0 (from omegaconf)\n",
            "  Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting click!=8.0.0,>=7.1 (from wandb)\n",
            "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in c:\\users\\nunzi\\appdata\\roaming\\python\\python312\\site-packages (from wandb) (4.2.2)\n",
            "Collecting protobuf!=4.21.0,<6,>=3.19.0 (from wandb)\n",
            "  Downloading protobuf-5.27.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\nunzi\\appdata\\roaming\\python\\python312\\site-packages (from wandb) (6.0.0)\n",
            "Collecting requests<3,>=2.0.0 (from wandb)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.13.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
            "Collecting setuptools (from wandb)\n",
            "  Using cached setuptools-73.0.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\nunzi\\appdata\\roaming\\python\\python312\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
            "Requirement already satisfied: six>=1.4.0 in c:\\users\\nunzi\\appdata\\roaming\\python\\python312\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs)\n",
            "  Downloading pyreadline3-3.4.1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.0.0->wandb)\n",
            "  Downloading charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3,>=2.0.0->wandb)\n",
            "  Downloading idna-3.8-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.0.0->wandb)\n",
            "  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3,>=2.0.0->wandb)\n",
            "  Downloading certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Downloading wandb-0.17.7-py3-none-win_amd64.whl (6.5 MB)\n",
            "   ---------------------------------------- 0.0/6.5 MB ? eta -:--:--\n",
            "   -------------------------------- ------- 5.2/6.5 MB 29.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 6.5/6.5 MB 25.0 MB/s eta 0:00:00\n",
            "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Downloading protobuf-5.27.3-cp310-abi3-win_amd64.whl (426 kB)\n",
            "Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
            "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading sentry_sdk-2.13.0-py2.py3-none-any.whl (309 kB)\n",
            "Downloading setproctitle-1.3.3-cp312-cp312-win_amd64.whl (11 kB)\n",
            "Using cached setuptools-73.0.1-py3-none-any.whl (2.3 MB)\n",
            "Downloading certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
            "Downloading charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl (100 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "Downloading idna-3.8-py3-none-any.whl (66 kB)\n",
            "Downloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
            "Downloading pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
            "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (pyproject.toml): started\n",
            "  Building wheel for antlr4-python3-runtime (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144578 sha256=022d2db5ad9dfc6660af0b1472b5ffec78bbe2a0a0febd39d6671300edb59b99\n",
            "  Stored in directory: c:\\users\\nunzi\\appdata\\local\\pip\\cache\\wheels\\1f\\be\\48\\13754633f1d08d1fbfc60d5e80ae1e5d7329500477685286cd\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: pyreadline3, antlr4-python3-runtime, urllib3, smmap, setuptools, setproctitle, PyYAML, protobuf, idna, humanfriendly, docker-pycreds, click, charset-normalizer, certifi, sentry-sdk, requests, omegaconf, gitdb, coloredlogs, gitpython, wandb\n",
            "Successfully installed PyYAML-6.0.2 antlr4-python3-runtime-4.9.3 certifi-2024.7.4 charset-normalizer-3.3.2 click-8.1.7 coloredlogs-15.0.1 docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 humanfriendly-10.0 idna-3.8 omegaconf-2.3.0 protobuf-5.27.3 pyreadline3-3.4.1 requests-2.32.3 sentry-sdk-2.13.0 setproctitle-1.3.3 setuptools-73.0.1 smmap-5.0.1 urllib3-2.2.2 wandb-0.17.7\n"
          ]
        }
      ],
      "source": [
        "# Installing missing dependencies\n",
        "!pip install omegaconf coloredlogs wandb"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EPIC-Kitchens-55 dataset\n",
        "\n",
        "**READ carefully!**\n",
        "\n",
        "To develop the project, you need to download the RGB frames for a subset of EPIC-Kitchens-55 (participants P08, P01 and P22) from [here](https://drive.google.com/drive/u/1/folders/1dJOtZ07WovP3YSCRAnU0E4gsfqDzpMVo). \n",
        "\n",
        "You also need to the pretrained checkpoints for each domain from [here](https://politoit-my.sharepoint.com/:f:/g/personal/simone_peirone_polito_it/ErdsZhvmR65Lun5_5O0-l5sBTPjCCZZq2f700Tj_CNzjTQ?e=L1yflf).\n",
        "\n",
        "Add the Google Drive directory containing the dataset to your Google Drive or upload the dataset on your Google Drive to access it from Google Colab.\n",
        "\n",
        "**NOTE**: As the dataset is quite heavy, we stronly suggest you to implement and test all your code on one for the three dataset. Then, once you are sure everything works, repeat the experiments on the remaining two datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# As read and write operations from google drive are slow, we suggest to copy and unzip\n",
        "# the dataset in a local directory on the Colab's machine.\n",
        "mkdir -p ek_data/frames\n",
        "\n",
        "# Copy the *.zip files of Epic-Kitchens\n",
        "cp /content/drive/MyDrive/epic_kitchens_reduced/P08_0*.zip ./ek_data\n",
        "# unzip\n",
        "for file in ./ek_data/*.zip; do\n",
        "  fn=$(basename $file)\n",
        "  fn=${fn/.zip}\n",
        "  ls -lah $file\n",
        "  echo \"Processing $file\"\n",
        "  mkdir -p ek_data/frames/$fn\n",
        "  unzip $file -d ek_data/frames/$fn\n",
        "done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Create directory for extracted frames\n",
        "os.makedirs('ek_data/frames', exist_ok=True)\n",
        "\n",
        "# Copy and unzip files from source directory\n",
        "source_dir = r'Epich_kitchens_reduced'\n",
        "target_dir = r'ek_data'\n",
        "\n",
        "# Unzipping the files\n",
        "for file in os.listdir(source_dir):\n",
        "    if file.endswith('.zip'):\n",
        "        file_path = os.path.join(source_dir, file)\n",
        "        print(f\"Processing {file_path}\")\n",
        "        \n",
        "        # Create a folder to extract the content\n",
        "        folder_name = file.replace('.zip', '')\n",
        "        output_dir = os.path.join(target_dir, 'frames', folder_name)\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        \n",
        "        # Extract the zip file\n",
        "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(output_dir)\n",
        "\n",
        "        print(f\"Finished extracting {file_path}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Features extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88YghJyXhbfS"
      },
      "outputs": [],
      "source": [
        "!python save_feat.py name=change_me config=configs/I3D_save_feat.yaml dataset.shift=D1-D1 dataset.RGB.data_path=.\\ek_data\\frames\n",
        "\n",
        "# If everything is working, you should expect an error message telling you to implement the '_get_val_indices' method in the dataset class.\n",
        "# Once you have implemented it, you should run the script for the train and test split of the dataset to extract the features."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMA44pwS84HIKtaEclSmH2W",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "aml22",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "2fc1f0eeae38a5df67b0f713e03196095ce1bfa55aa551576e8e58c2ba904c5a"
      }
    }
>>>>>>> b2f6c1292a809c6035d72ab90d5da340f9108111
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/EgovisionPolito/aml23-ego/blob/master/colab_runner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the public repository (or your repository)\n",
    "!git clone https://github.com/EgovisionPolito/aml23-ego.git aml23-ego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing missing dependencies\n",
    "!pip install omegaconf coloredlogs wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPIC-Kitchens-55 dataset\n",
    "\n",
    "**READ carefully!**\n",
    "\n",
    "To develop the project, you need to download the RGB frames for a subset of EPIC-Kitchens-55 (participants P08, P01 and P22) from [here](https://drive.google.com/drive/u/1/folders/1dJOtZ07WovP3YSCRAnU0E4gsfqDzpMVo). \n",
    "\n",
    "You also need to the pretrained checkpoints for each domain from [here](https://politoit-my.sharepoint.com/:f:/g/personal/simone_peirone_polito_it/ErdsZhvmR65Lun5_5O0-l5sBTPjCCZZq2f700Tj_CNzjTQ?e=L1yflf).\n",
    "\n",
    "Add the Google Drive directory containing the dataset to your Google Drive or upload the dataset on your Google Drive to access it from Google Colab.\n",
    "\n",
    "**NOTE**: As the dataset is quite heavy, we stronly suggest you to implement and test all your code on one for the three dataset. Then, once you are sure everything works, repeat the experiments on the remaining two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/76/5ysc1qcx51g2dmdfhmc1f3fw0000gn/T/ipykernel_24303/859467756.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Mount google drive\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolab\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdrive\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mdrive\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'/content/drive'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Mount google drive \n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: cp [-R [-H | -L | -P]] [-fi | -n] [-aclpSsvXx] source_file target_file\n",
      "       cp [-R [-H | -L | -P]] [-fi | -n] [-aclpSsvXx] source_file ... target_directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--@ 1 raffaeleviola  staff   122M Dec 31  1979 ./ek_data/P08_02.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff   168M Dec 31  1979 ./ek_data/P08_03.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff   237M Dec 31  1979 ./ek_data/P08_04.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff   584M Dec 31  1979 ./ek_data/P08_06.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff    24M Dec 31  1979 ./ek_data/P08_07.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff   161M Dec 31  1979 ./ek_data/P08_08.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff   821M Dec 31  1979 ./ek_data/P08_09.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff   290M Dec 31  1979 ./ek_data/P08_10.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff   206M Dec 31  1979 ./ek_data/P08_11.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff    27M Dec 31  1979 ./ek_data/P08_12.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff    21M Dec 31  1979 ./ek_data/P08_13.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff   170M Dec 31  1979 ./ek_data/P08_14.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff   664M Dec 31  1979 ./ek_data/P08_15.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff   785M Dec 31  1979 ./ek_data/P08_16.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff   796M Dec 31  1979 ./ek_data/P08_17.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff   577M Dec 31  1979 ./ek_data/P08_18.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff   151M Dec 31  1979 ./ek_data/P08_19.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff   244M Dec 31  1979 ./ek_data/P08_20.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff   1.8G Dec 31  1979 ./ek_data/P08_21.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff   147M Dec 31  1979 ./ek_data/P08_22.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff   846M Dec 31  1979 ./ek_data/P08_24.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff    29M Dec 31  1979 ./ek_data/P08_25.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff   497M Dec 31  1979 ./ek_data/P08_26.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff   218M Dec 31  1979 ./ek_data/P08_27.zip\n",
      "-rw-r--r--@ 1 raffaeleviola  staff    34M Dec 31  1979 ./ek_data/P08_28.zip\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# As read and write operations from google drive are slow, we suggest to copy and unzip\n",
    "# the dataset in a local directory on the Colab's machine.\n",
    "mkdir -p ek_data/frames\n",
    "\n",
    "# Copy the *.tar.gz files of Epic-Kitchens\n",
    "# TODO: replace with your path to the dataset\n",
    "# cp /content/drive/MyDrive/AML23/EPIC-Kitchens-55/data/*.tar.gz ./ek_data\n",
    "cp /Users/raffaeleviola/Documents/AMLProject/EK_Data\n",
    "# Untar\n",
    "for file in ./ek_data/*.zip; do\n",
    "  fn=$(basename $file)\n",
    "  fn=${fn/.zip/}\n",
    "  ls -lah $file\n",
    "  tar xf $file --directory=ek_data/frames\n",
    "done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "88YghJyXhbfS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/raffaeleviola/Documents/AMLProject/save_feat.py\", line 2, in <module>\n",
      "    from utils.logger import logger\n",
      "  File \"/Users/raffaeleviola/Documents/AMLProject/utils/logger.py\", line 2, in <module>\n",
      "    import coloredlogs\n",
      "ModuleNotFoundError: No module named 'coloredlogs'\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b\"\\n# Replace the following path with the path of your codebase\\n# cd aml23-ego\\n\\npython save_feat.py name=change_me \\\\\\n  config=configs/I3D_save_feat.yaml \\\\\\n  dataset.shift=D1-D1 \\\\\\n  dataset.RGB.data_path=../ek_data/frames \\n\\n# If everything is working, you should expect an error message telling you to implement the '_get_val_indices' method in the dataset class.\\n# Once you have implemented it, you should run the script for the train and test split of the dataset to extract the features.\\n\"' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mCalledProcessError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_cell_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbash\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m# Replace the following path with the path of your codebase\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m# cd aml23-ego\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mpython save_feat.py name=change_me \u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m  config=configs/I3D_save_feat.yaml \u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m  dataset.shift=D1-D1 \u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m  dataset.RGB.data_path=../ek_data/frames \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m# If everything is working, you should expect an error message telling you to implement the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_get_val_indices\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m method in the dataset class.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m# Once you have implemented it, you should run the script for the train and test split of the dataset to extract the features.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2517\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n\u001B[1;32m   2515\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m   2516\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n\u001B[0;32m-> 2517\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   2519\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[1;32m   2520\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n\u001B[1;32m   2521\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[1;32m   2522\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/IPython/core/magics/script.py:154\u001B[0m, in \u001B[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001B[0;34m(line, cell)\u001B[0m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    153\u001B[0m     line \u001B[38;5;241m=\u001B[39m script\n\u001B[0;32m--> 154\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshebang(line, cell)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/IPython/core/magics/script.py:314\u001B[0m, in \u001B[0;36mScriptMagics.shebang\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m args\u001B[38;5;241m.\u001B[39mraise_error \u001B[38;5;129;01mand\u001B[39;00m p\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    310\u001B[0m     \u001B[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001B[39;00m\n\u001B[1;32m    311\u001B[0m     \u001B[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001B[39;00m\n\u001B[1;32m    312\u001B[0m     \u001B[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001B[39;00m\n\u001B[1;32m    313\u001B[0m     rc \u001B[38;5;241m=\u001B[39m p\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m9\u001B[39m\n\u001B[0;32m--> 314\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CalledProcessError(rc, cell)\n",
      "\u001B[0;31mCalledProcessError\u001B[0m: Command 'b\"\\n# Replace the following path with the path of your codebase\\n# cd aml23-ego\\n\\npython save_feat.py name=change_me \\\\\\n  config=configs/I3D_save_feat.yaml \\\\\\n  dataset.shift=D1-D1 \\\\\\n  dataset.RGB.data_path=../ek_data/frames \\n\\n# If everything is working, you should expect an error message telling you to implement the '_get_val_indices' method in the dataset class.\\n# Once you have implemented it, you should run the script for the train and test split of the dataset to extract the features.\\n\"' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Replace the following path with the path of your codebase\n",
    "# cd aml23-ego\n",
    "\n",
    "python save_feat.py name=change_me \\\n",
    "  config=configs/I3D_save_feat.yaml \\\n",
    "  dataset.shift=D1-D1 \\\n",
    "  dataset.RGB.data_path=../ek_data/frames \n",
    "\n",
    "# If everything is working, you should expect an error message telling you to implement the '_get_val_indices' method in the dataset class.\n",
    "# Once you have implemented it, you should run the script for the train and test split of the dataset to extract the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMA44pwS84HIKtaEclSmH2W",
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "aml22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2fc1f0eeae38a5df67b0f713e03196095ce1bfa55aa551576e8e58c2ba904c5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
